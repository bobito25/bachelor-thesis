# config.conf

# Positional Argument
# MODEL_NAME="Qwen/Qwen2.5-14B-Instruct"
# MODEL_NAME="Qwen/Qwen2.5-32B-Instruct-GPTQ-Int8"
# MODEL_NAME="Qwen/Qwen3-14B"
MODEL_NAME="openai/gpt-oss-20b"
# Parameters with Values
PARAMS=(
  "--max-model-len" "14000"  #Change that to load bigger model
  "--host" "0.0.0.0"
  "--gpu-memory-utilization" "0.9" #Better to set this below 1 to have some memory for other processes
  "--port" "8000" #No need to change that
  # "--limit-mm-per-prompt" "image=1" #Would set this for VLMs 
  "--tensor-parallel-size" "4" #Means it uses X GPUs basically
  #"--tokenizer" "Qwen/Qwen3-14B"  # Use original tokenizer for GGUF models
  # "--pipeline-parallel-size" "4" # How many Nodes to use
  # "--quantization" "moe_wna16" # Hosting R1 with 4 bit quantization
  # "--tokenizer_mode" "mistral"
  # "--max_num_seqs " "1" #Number of sequences to generate
)

# Flags (Parameters without Values)
FLAGS=(
  "--trust-remote-code"
  "--disable-frontend-multiprocessing"
  "--enforce-eager"
)

